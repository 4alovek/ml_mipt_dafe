{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Seminar_5 - kaggle practice.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06c9a1357a7a4378b5d8c71660a24273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_230c4bc49226493d8aacbdf0740c976c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae77f92738a54442a5e773e1debad2b2",
              "IPY_MODEL_51ad3f0edaa9421d9b13b42a4c7924a6",
              "IPY_MODEL_7f983a2928b8461fa7456ff8b09da5df"
            ]
          }
        },
        "230c4bc49226493d8aacbdf0740c976c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae77f92738a54442a5e773e1debad2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de9547e395a84a7385cfeeb4314d73e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91f1101e70bf466e84e9cc45f8d73eec"
          }
        },
        "51ad3f0edaa9421d9b13b42a4c7924a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9ab0e0e388d43ad8258c1c8a05f6ae2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6fdb33a6264b466980d508c19cbc94b2"
          }
        },
        "7f983a2928b8461fa7456ff8b09da5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1b4267e082904ba1a1afe4ad788e50c3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 123MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_522a83c6310e43f28539db312f776e8e"
          }
        },
        "de9547e395a84a7385cfeeb4314d73e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91f1101e70bf466e84e9cc45f8d73eec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9ab0e0e388d43ad8258c1c8a05f6ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6fdb33a6264b466980d508c19cbc94b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b4267e082904ba1a1afe4ad788e50c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "522a83c6310e43f28539db312f776e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gvTkvNIoqmD",
        "outputId": "2067374a-7a6d-4b14-c637-8f1d8b124179"
      },
      "source": [
        "!pip install  torchtoolbox #efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtoolbox in /usr/local/lib/python3.7/dist-packages (0.1.8.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (2.8.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (6.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (4.1.2.30)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (0.99)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from torchtoolbox) (6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->torchtoolbox) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->torchtoolbox) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->torchtoolbox) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->torchtoolbox) (3.7.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchtoolbox) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->torchtoolbox) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchtoolbox) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtoolbox) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtoolbox) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchtoolbox) (3.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->torchtoolbox) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->torchtoolbox) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->torchtoolbox) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->torchtoolbox) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->torchtoolbox) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->torchtoolbox) (0.0.47)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->torchtoolbox) (3.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->torchtoolbox) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "feErM2jhoqmD"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "#import albumentations as A\n",
        "#from albumentations.pytorch.transforms import ToTensor\n",
        "import torchtoolbox.transform as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import datetime\n",
        "import warnings\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nGdqqmnDoqmD"
      },
      "source": [
        "warnings.simplefilter('ignore')\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(47)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "es0ko153oqmD"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_c-Kc7afKBj"
      },
      "source": [
        "# [Melanoma classification](https://www.kaggle.com/c/siim-isic-melanoma-classification/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbinOH3tsuDw"
      },
      "source": [
        "# 1 Посмотрим на данные"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hQlWKRUEJsh",
        "outputId": "920dee72-df87-44b8-ea44-f8f1febcc3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In9OWBXwssOF"
      },
      "source": [
        "train_df = pd.read_csv(\"./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/train.csv\")\n",
        "test_df = pd.read_csv(\"./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd88VHMA0ZfU"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL2KvK2KDWi8"
      },
      "source": [
        "test_df.info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The total patient ids are {train_df['patient_id'].count()}, from those the unique ids are {train_df['patient_id'].value_counts().shape[0]} \")"
      ],
      "metadata": {
        "id": "aLBkTcChLdvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['target'].value_counts()"
      ],
      "metadata": {
        "id": "qDqo8ODhLrlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGZfqGAyy1kE"
      },
      "source": [
        "train_images = \"./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/train\"\n",
        "test_images = \"./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of images in the dataset(train+test)\n",
        "print(\"Total images in Train set: \",train_df['image_name'].count())\n",
        "print(\"Total images in Test set: \",test_df['image_name'].count())"
      ],
      "metadata": {
        "id": "Ss562MplLjCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBfz-W-e01MA"
      },
      "source": [
        "im_path = str(train_images + \"/\" + train_df.iloc[0]['image_name'] + '.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Sej7Kp8L01Oj",
        "outputId": "78488b77-1a72-4d11-e7c2-8ab2fc72910f"
      },
      "source": [
        "im_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/train/ISIC_2637011.jpg'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBJ9PdSAiON"
      },
      "source": [
        "x = cv2.imread(im_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfgfYBJaBQpd"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def disp_img(dataframe):\n",
        "  images = dataframe['image_name'].values\n",
        "\n",
        "  # Extract 9 random images from it\n",
        "  random_images = [np.random.choice(images+'.jpg') for i in range(9)]\n",
        "\n",
        "  print('Display Random Images')\n",
        "\n",
        "  # Adjust the size of your images\n",
        "  plt.figure(figsize=(10,8))\n",
        "\n",
        "  # Iterate and plot random images\n",
        "  for i in range(9):\n",
        "      plt.subplot(3, 3, i + 1)\n",
        "      img = plt.imread(os.path.join(train_images, random_images[i]))\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      plt.axis('off')\n",
        "      \n",
        "  # Adjust subplot parameters to give specified padding\n",
        "  plt.tight_layout()  "
      ],
      "metadata": {
        "id": "SM2K5EXQRi9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp_img(train_df)"
      ],
      "metadata": {
        "id": "yIHzyEezQx_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benign = train_df[train_df['benign_malignant']=='benign']\n",
        "malignant = train_df[train_df['benign_malignant']=='malignant']"
      ],
      "metadata": {
        "id": "ZGHCQKtuROot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp_img(benign)"
      ],
      "metadata": {
        "id": "brxR3B4TR_yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp_img(malignant)"
      ],
      "metadata": {
        "id": "sSiZBd_4RbBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Три пути решения задачи\n",
        "- Детерминированные подходы на основе формализованных правил + статистика\n",
        "- [Пусть нейросети сами как-то](https://www.kaggle.com/saife245/melanoma-detail-analysis-eda-ip-augmentation-model)\n",
        "- Нейросети + помощь от разработчика"
      ],
      "metadata": {
        "id": "BXtHHbgDij8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 На этом этапе стоит обратиться к [доменной области](https://www.kaggle.com/saife245/melanoma-detail-analysis-eda-ip-augmentation-model) для большего понимания проблемы"
      ],
      "metadata": {
        "id": "sdGP7z65fyrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Что делать с метаданными и какие аугментации применять?**"
      ],
      "metadata": {
        "id": "zLTXO6oOIoDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Нейросетевой подход. Подготовка данных"
      ],
      "metadata": {
        "id": "FPIUY0Y-i9b2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_4NHLUrwPIM"
      },
      "source": [
        "## 4.1 Предобработка Meta данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LL2iDvnsoqmD"
      },
      "source": [
        "# One-hot encoding of anatom_site_general_challenge feature\n",
        "concat = pd.concat([train_df['anatom_site_general_challenge'], test_df['anatom_site_general_challenge']], ignore_index=True)\n",
        "\n",
        "dummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix='site')\n",
        "\n",
        "train_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\n",
        "test_df = pd.concat([test_df, dummies.iloc[train_df.shape[0]:].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Sex features\n",
        "train_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\n",
        "test_df['sex'] = test_df['sex'].map({'male': 1, 'female': 0})\n",
        "\n",
        "train_df['sex'] = train_df['sex'].fillna(-1)\n",
        "test_df['sex'] = test_df['sex'].fillna(-1)\n",
        "\n",
        "# Age features\n",
        "train_df['age_approx'] /= train_df['age_approx'].max()\n",
        "test_df['age_approx'] /= test_df['age_approx'].max()\n",
        "\n",
        "train_df['age_approx'] = train_df['age_approx'].fillna(0)\n",
        "test_df['age_approx'] = test_df['age_approx'].fillna(0)\n",
        "\n",
        "train_df['patient_id'] = train_df['patient_id'].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x6n5F4RXoqmD"
      },
      "source": [
        "meta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\n",
        "meta_features.remove('anatom_site_general_challenge')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "petsUhkd5Vcr"
      },
      "source": [
        "meta_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ_IUNhD8PUY"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MURKhl82uE3s"
      },
      "source": [
        "## 4.2 Создадим Датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "poOKh67PoqmD"
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "    def __init__()\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "    \n",
        "    def __len__(self):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1cu6CNIu8Ro"
      },
      "source": [
        "## 4.3 Задаем преобразования"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1 Классические аугментации"
      ],
      "metadata": {
        "id": "iXDPtdswjMMl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kz-Yn4rpoqmD"
      },
      "source": [
        "train_transform = \n",
        "\n",
        "test_transform = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geoD1JZ9zE0C"
      },
      "source": [
        "test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1VhxDu6Z-c"
      },
      "source": [
        "train_idx = [1, 2, 3]\n",
        "val_idx = [4, 5, 6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doETYUQYzc9B"
      },
      "source": [
        "train = \n",
        "val = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaT774I-fd_g",
        "outputId": "cf6a3829-7930-4bd8-805b-dfa84e326791"
      },
      "source": [
        "train.__len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 Волосы"
      ],
      "metadata": {
        "id": "M_aurCVojjgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавление волос"
      ],
      "metadata": {
        "id": "0muntTp3sOzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedHairAugmentation:\n",
        "    \"\"\"\n",
        "    Impose an image of a hair to the target image\n",
        "\n",
        "    Args:\n",
        "        hairs (int): maximum number of hairs to impose\n",
        "        hairs_folder (str): path to the folder with hairs images\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n",
        "        self.hairs = hairs\n",
        "        self.hairs_folder = hairs_folder\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to draw hairs on.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image: Image with drawn hairs.\n",
        "        \"\"\"\n",
        "        n_hairs = random.randint(0, self.hairs)\n",
        "        \n",
        "        if not n_hairs:\n",
        "            return img\n",
        "        \n",
        "        height, width, _ = img.shape  # target image width and height\n",
        "        hair_images = [im for im in os.listdir(self.hairs_folder) if 'png' in im]\n",
        "        \n",
        "        for _ in range(n_hairs):\n",
        "            hair = cv2.imread(os.path.join(self.hairs_folder, random.choice(hair_images)))\n",
        "            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n",
        "            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n",
        "\n",
        "            h_height, h_width, _ = hair.shape  # hair image width and height\n",
        "            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n",
        "            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n",
        "            roi = img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width]\n",
        "\n",
        "            # Creating a mask and inverse mask\n",
        "            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n",
        "            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n",
        "            mask_inv = cv2.bitwise_not(mask)\n",
        "\n",
        "            # Now black-out the area of hair in ROI\n",
        "            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n",
        "\n",
        "            # Take only region of hair from hair image.\n",
        "            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n",
        "\n",
        "            # Put hair in ROI and modify the target image\n",
        "            dst = cv2.add(img_bg, hair_fg)\n",
        "\n",
        "            img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width] = dst\n",
        "                \n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'"
      ],
      "metadata": {
        "id": "3ckQEKHKpcmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.3 Микроскоп"
      ],
      "metadata": {
        "id": "seyTUo_njt-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обрезание по кругу"
      ],
      "metadata": {
        "id": "epuxGCUdto1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Microscope:\n",
        "    \"\"\"\n",
        "    Cutting out the edges around the center circle of the image\n",
        "    Imitating a picture, taken through the microscope\n",
        "\n",
        "    Args:\n",
        "        p (float): probability of applying an augmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to apply transformation to.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image: Image with transformation.\n",
        "        \"\"\"\n",
        "        if random.random() < self.p:\n",
        "            circle = cv2.circle((np.ones(img.shape) * 255).astype(np.uint8), # image placeholder\n",
        "                        (img.shape[0]//2, img.shape[1]//2), # center point of circle\n",
        "                        random.randint(img.shape[0]//2 - 3, img.shape[0]//2 + 15), # radius\n",
        "                        (0, 0, 0), # color\n",
        "                        -1)\n",
        "\n",
        "            mask = circle - 255\n",
        "            img = np.multiply(img, mask)\n",
        "        \n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(p={self.p})'"
      ],
      "metadata": {
        "id": "jjCzw1bVpZCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.4 Масштабирование"
      ],
      "metadata": {
        "id": "C4LK_1Ohm2zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Идеи:"
      ],
      "metadata": {
        "id": "qC2dvgrnnKhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.5 Анализ последовательности снимков"
      ],
      "metadata": {
        "id": "cRUhai_Dm-li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Идеи:"
      ],
      "metadata": {
        "id": "ek4h6iwNnT9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.6 Пересчет параметров нормализации"
      ],
      "metadata": {
        "id": "-ingMc5mrjD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часто может помочь пересчитать матожидание и дисперсию по каждому каналу для текущих данных"
      ],
      "metadata": {
        "id": "RXVS5yFRro2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.7 Обновим преобразования"
      ],
      "metadata": {
        "id": "MewiGs8XrXVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = \n",
        "test_transform = "
      ],
      "metadata": {
        "id": "mrsmYBnJpijC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXn9dEWUurJc"
      },
      "source": [
        "# 5 Подготовка и обучение нейросети"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Собираем архитектуру"
      ],
      "metadata": {
        "id": "qj-j3qMilSgc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "06c9a1357a7a4378b5d8c71660a24273",
            "230c4bc49226493d8aacbdf0740c976c",
            "ae77f92738a54442a5e773e1debad2b2",
            "51ad3f0edaa9421d9b13b42a4c7924a6",
            "7f983a2928b8461fa7456ff8b09da5df",
            "de9547e395a84a7385cfeeb4314d73e6",
            "91f1101e70bf466e84e9cc45f8d73eec",
            "f9ab0e0e388d43ad8258c1c8a05f6ae2",
            "6fdb33a6264b466980d508c19cbc94b2",
            "1b4267e082904ba1a1afe4ad788e50c3",
            "522a83c6310e43f28539db312f776e8e"
          ]
        },
        "id": "5y-wbPUExmdb",
        "outputId": "eddfb2ab-0ccf-4d02-e2f7-253494f3f4c5"
      },
      "source": [
        "resnet18 = models.resnet18(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06c9a1357a7a4378b5d8c71660a24273",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs_YXM8Dxnu-",
        "outputId": "cb705936-edea-4fc5-f2af-300f8a8f84b4"
      },
      "source": [
        "resnet18.fc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=1000, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3afKG_wis2-x"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, arch, n_meta_features: int):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        No sigmoid in forward because we are going to use BCEWithLogitsLoss\n",
        "        Which applies sigmoid for us when calculating a loss\n",
        "        \"\"\"\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0G-IJ5b7CYp"
      },
      "source": [
        "model = Net(arch=resnet18, n_meta_features=len(meta_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCI8eLFM7N1u",
        "outputId": "94297fdf-7e61-4669-d4bf-77da4119283a"
      },
      "source": [
        "params = list(model.parameters())\n",
        "print(len(params))\n",
        "print(params[71])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n",
            "Parameter containing:\n",
            "tensor([-0.0210], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giMLX6bzwq9k"
      },
      "source": [
        "## 5.2 Обучим модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f7FzHS9RwH"
      },
      "source": [
        "epochs = 3  # Number of epochs to run\n",
        "es_patience = 3  # Early Stopping patience - for how many epochs with no improvements to wait\n",
        "TTA = 1 # Test Time Augmentation rounds\n",
        "\n",
        "oof = np.zeros((len(train_df), 1))  # Out Of Fold predictions\n",
        "preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)  # Predictions for test test\n",
        "\n",
        "skf = KFold(n_splits=2, shuffle=True, random_state=47)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8HXBJ_9Kxyv"
      },
      "source": [
        "### 5.2.1 Создадим train/val/test подвыборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvNZVAe_DqVM"
      },
      "source": [
        "train_len = 500\n",
        "train_df = train_df.iloc[:train_len]\n",
        "\n",
        "train_idx = np.linspace(0, train_len-51, num=train_len-50, dtype=int)\n",
        "val_idx = np.linspace(train_len-50, train_len-1, num=50, dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnO31_8ZJrB1"
      },
      "source": [
        "train = MelanomaDataset(df=train_df.iloc[train_idx].reset_index(drop=True), \n",
        "                        imfolder=train_images, \n",
        "                        train=True, \n",
        "                        transforms=train_transform,\n",
        "                        meta_features=meta_features)\n",
        "\n",
        "val = MelanomaDataset(df=train_df.iloc[val_idx].reset_index(drop=True), \n",
        "                        imfolder=train_images, \n",
        "                        train=True, \n",
        "                        transforms=test_transform,\n",
        "                        meta_features=meta_features)\n",
        "\n",
        "test = MelanomaDataset(df=test_df.iloc[:500], # изменить здесь, чтобы сделать сабмит\n",
        "                       imfolder=test_images, \n",
        "                       train=False,\n",
        "                       transforms=train_transform,  # For TTA\n",
        "                       meta_features=meta_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJF7D7SyK9vP"
      },
      "source": [
        "train_loader = DataLoader(dataset=train, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic0DVLdrLFda"
      },
      "source": [
        "### 5.2.2 Создадим модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiABKx2CJq-2"
      },
      "source": [
        "best_val = 0  # Best validation score within this fold\n",
        "patience = es_patience  # Current patience counter\n",
        "\n",
        "arch = models.resnet18(pretrained=True)\n",
        "model = Net(arch=arch, n_meta_features=len(meta_features))  # New model for each fold\n",
        "model = model.to(device)\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op6HvirWLiNT"
      },
      "source": [
        "### 5.2.3 Цикл обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMHe9unyJrEq",
        "outputId": "2156bcf6-1740-4262-9182-94dba8bed7c8"
      },
      "source": [
        "correct = 0\n",
        "epoch_loss = 0\n",
        "\n",
        "for x, y in train_loader:\n",
        "  ##################### достаем батч #####################\n",
        "  x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n",
        "  x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n",
        "  y = torch.tensor(y, device=device, dtype=torch.float32)\n",
        "\n",
        "  ##################### считаем ошибку #####################\n",
        "  optim.zero_grad()\n",
        "  z = model(x)\n",
        "  loss = criterion(z, y.unsqueeze(1))\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n",
        "  correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()  # tracking number of correctly predicted samples\n",
        "  epoch_loss += loss.item()\n",
        "\n",
        "train_acc = correct / len(train_idx)\n",
        "\n",
        "print(epoch_loss, train_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.010782007858225 0.976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeF_Jd83WCJC"
      },
      "source": [
        "### 5.2.4 Валидация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ia0rEf2MJ6t",
        "outputId": "f3c106ee-b0e3-4aff-ae32-a197dc60aec9"
      },
      "source": [
        "val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
        "model_path = './gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/models/model.pth'  # Path and filename to save model to\n",
        "with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
        "    # Predicting on validation set\n",
        "    for j, (x_val, y_val) in enumerate(val_loader):\n",
        "      ##################### достаем батч #####################\n",
        "        x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
        "        x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
        "        y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
        "\n",
        "        ##################### считаем точность на валидации #####################\n",
        "        z_val = model(x_val)\n",
        "        val_pred = torch.sigmoid(z_val)\n",
        "        val_preds[j*val_loader.batch_size:j*val_loader.batch_size + x_val[0].shape[0]] = val_pred\n",
        "\n",
        "    val_acc = accuracy_score(train_df.iloc[val_idx]['target'].values, torch.round(val_preds.cpu()))\n",
        "    val_roc = roc_auc_score(train_df.iloc[val_idx]['target'].values, val_preds.cpu())\n",
        "    \n",
        "    print(val_acc, val_roc)\n",
        "\n",
        "    scheduler.step(val_roc)\n",
        "        \n",
        "    if val_roc >= best_val:\n",
        "        best_val = val_roc\n",
        "        patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
        "        torch.save(model, model_path)  # Saving current best model\n",
        "    else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "            print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.988 0.5195681511470984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOxUDD4TMK0T"
      },
      "source": [
        " ### 5.2.5 Сделаем предсказание"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taLEIwsBXgE3",
        "outputId": "1347d89b-633c-4773-82d7-52fb0d5e8da9"
      },
      "source": [
        "model.eval()  # switch model to the evaluation mode\n",
        "preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)  # Predictions for test\n",
        "TTA = 3 # Test Time Augmentation rounds\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Predicting on test set\n",
        "    tta_preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n",
        "    for t in range(TTA):\n",
        "        print( '  TTA', t)\n",
        "\n",
        "        for i, x_test in enumerate(tqdm(test_loader)):\n",
        "            x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n",
        "            x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n",
        "            \n",
        "            z_test = model(x_test)\n",
        "            \n",
        "            z_test = torch.sigmoid(z_test)\n",
        "            \n",
        "            tta_preds[i*test_loader.batch_size:i*test_loader.batch_size + x_test[0].shape[0]] += z_test\n",
        "    preds += tta_preds / TTA\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TTA 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TTA 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TTA 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:11<00:00,  2.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxOVFn2DXi-I"
      },
      "source": [
        "### 5.2.6 Обучение на K фолдах (stratified k fold)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f5aiMBFoqmD",
        "outputId": "0ec4e3f1-7722-4c4c-89b7-e9d7eae6fca8"
      },
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X=np.zeros(len(train_df)), y=train_df['target'], groups=train_df['patient_id'].tolist()), 1):\n",
        "    print('=' * 20, 'Fold', fold, '=' * 20)  \n",
        "    \n",
        "    ##################### создаем модель #####################\n",
        "    model_path = f'./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/models/model_{fold}.pth'  # Path and filename to save model to\n",
        "    best_val = 0  # Best validation score within this fold\n",
        "    patience = es_patience  # Current patience counter\n",
        "\n",
        "    arch = models.resnet18(pretrained=True)\n",
        "    model = Net(arch=arch, n_meta_features=len(meta_features))  # New model for each fold\n",
        "    model = model.to(device)\n",
        "    \n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.2)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    ##################### выделяем валидационную выборку #####################\n",
        "    train = MelanomaDataset(df=train_df.iloc[train_idx].reset_index(drop=True), \n",
        "                            imfolder=train_images, \n",
        "                            train=True, \n",
        "                            transforms=train_transform,\n",
        "                            meta_features=meta_features)\n",
        "    val = MelanomaDataset(df=train_df.iloc[val_idx].reset_index(drop=True), \n",
        "                            imfolder=train_images, \n",
        "                            train=True, \n",
        "                            transforms=test_transform,\n",
        "                            meta_features=meta_features)\n",
        "    \n",
        "    train_loader = DataLoader(dataset=train, batch_size=16, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    ##################### цикл обучения #####################\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        correct = 0\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        \n",
        "        ##################### одна эпоха обучения #####################\n",
        "        for x, y in train_loader:\n",
        "            ##################### достаем батч #####################\n",
        "            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n",
        "            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n",
        "            y = torch.tensor(y, device=device, dtype=torch.float32)\n",
        "\n",
        "            ##################### считаем ошибку #####################\n",
        "            optim.zero_grad()\n",
        "            z = model(x)\n",
        "            loss = criterion(z, y.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n",
        "            correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()  # tracking number of correctly predicted samples\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        ##################### считаем точность на тренировке #####################\n",
        "        train_acc = correct / len(train_idx)\n",
        "        \n",
        "        model.eval()  # switch model to the evaluation mode\n",
        "        val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
        "        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
        "            # Predicting on validation set\n",
        "            for j, (x_val, y_val) in enumerate(val_loader):\n",
        "              ##################### достаем батч #####################\n",
        "                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
        "                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
        "                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
        "\n",
        "                ##################### считаем точность на валидации #####################\n",
        "                z_val = model(x_val)\n",
        "                val_pred = torch.sigmoid(z_val)\n",
        "                val_preds[j*val_loader.batch_size:j*val_loader.batch_size + x_val[0].shape[0]] = val_pred\n",
        "            val_acc = accuracy_score(train_df.iloc[val_idx]['target'].values, torch.round(val_preds.cpu()))\n",
        "            val_roc = roc_auc_score(train_df.iloc[val_idx]['target'].values, val_preds.cpu())\n",
        "            \n",
        "            print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n",
        "            epoch + 1, \n",
        "            epoch_loss, \n",
        "            train_acc, \n",
        "            val_acc, \n",
        "            val_roc, \n",
        "            str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n",
        "            \n",
        "            scheduler.step(val_roc)\n",
        "                \n",
        "            if val_roc >= best_val:\n",
        "                best_val = val_roc\n",
        "                patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
        "                torch.save(model, model_path)  # Saving current best model\n",
        "            else:\n",
        "                patience -= 1\n",
        "                if patience == 0:\n",
        "                    print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n",
        "                    break\n",
        "       \n",
        "    model = torch.load(model_path)  # Loading best model of this fold\n",
        "    model.eval()  # switch model to the evaluation mode\n",
        "    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
        "    with torch.no_grad():\n",
        "      \n",
        "        # Predicting on test set\n",
        "        tta_preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n",
        "        for _ in range(TTA):\n",
        "            for i, x_test in enumerate(test_loader):\n",
        "                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n",
        "                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n",
        "                z_test = model(x_test)\n",
        "                z_test = torch.sigmoid(z_test)\n",
        "                tta_preds[i*test_loader.batch_size:i*test_loader.batch_size + x_test[0].shape[0]] += z_test\n",
        "        preds += tta_preds / TTA\n",
        "    \n",
        "preds /= skf.n_splits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== Fold 1 ====================\n",
            "Epoch 001: | Loss: 2.484 | Train acc: 0.968 | Val acc: 0.988 | Val roc_auc: 0.684 | Training time: 0:00:15\n",
            "==================== Fold 2 ====================\n",
            "Epoch 001: | Loss: 2.437 | Train acc: 0.968 | Val acc: 0.988 | Val roc_auc: 0.447 | Training time: 0:00:08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSpDTaOtwFKB"
      },
      "source": [
        "## Делаем submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "tB2veBcLoqmD",
        "outputId": "c7cd6cce-cf43-468a-a3a4-a5d4cb1b83c9"
      },
      "source": [
        "sns.kdeplot(pd.Series(preds.cpu().numpy().reshape(-1,)));"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "vu_SsrPOoqmD",
        "outputId": "bce5d3f3-9a60-4583-d258-142a307d77d0"
      },
      "source": [
        "sub = pd.read_csv('./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/sample_submission.csv')\n",
        "sub['target'] = preds.cpu().numpy().reshape(-1,)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-a78d132cb58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 06 - kaggle practice/data/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         raise ValueError(\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (500) does not match length of index (10982)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odir9ppCdhLe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}