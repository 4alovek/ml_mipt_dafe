{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb17ef54-d7f3-4692-9dcb-59d4fabc47f6",
   "metadata": {},
   "source": [
    "## Practice #13 - A Visual Notebook to Using BERT for the First Time\n",
    "\n",
    "*Credits: first part of this notebook belongs to Jay Alammar and his [great blog post](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) (while it has minor changes). His blog is a great way to dive into the DL and NLP concepts.*\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png\" />\n",
    "\n",
    "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of sentences from film reviews. And we will calssify each sentence as either speaking \"positively\" about its subject of \"negatively\".\n",
    "\n",
    "### Models: Sentence Sentiment Classification\n",
    "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
    "\n",
    "Under the hood, the model is actually made up of two model.\n",
    "\n",
    "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
    "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n",
    "\n",
    "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />\n",
    "\n",
    "## Dataset\n",
    "The dataset we will use in this example is russian proverbs.\n",
    "\n",
    "## Installing the transformers library\n",
    "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3601fc5-55db-40f3-9867-f8a70d55b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85224c8d-1640-4737-a373-27b17c46b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70009671-3d7a-46f4-a3a4-70cc2af68528",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1. Using BERT for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d64362-da82-46a4-b69f-6fe404c7639d",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained BERT model\n",
    "Let's now load a pre-trained BERT model. Download [DeepPavlov Conversational DistilRuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6268d-d62e-42d3-a1da-63dcb5288703",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"./distil_ru_conversational_cased_L-6_H-768_A-12_pt/\")\n",
    "model = DistilBertModel.from_pretrained(\"./distil_ru_conversational_cased_L-6_H-768_A-12_pt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57624f-3486-4c10-92a4-cc5bdf7dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', './distil_ru_conversational_cased_L-6_H-768_A-12_pt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d612785-e2cf-4945-982b-57c25f683e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker(\"Привет, [MASK] зовут [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3792bb-8e0b-45e1-8936-8f704ac84bfd",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "We'll use pandas to read the dataset and load it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee2e07-c393-4380-9de8-f6d727f56874",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = 'ru_proverbs.txt'\n",
    "\n",
    "with open(input_data_path, 'r') as f:\n",
    "    proverbs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ccf43-548f-449f-8ff2-dfcecfe757b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550daae-0021-4fe6-a0be-f00013847674",
   "metadata": {},
   "source": [
    "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
    "\n",
    "### Step #1: Preparing the Dataset\n",
    "Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n",
    "\n",
    "### Tokenization\n",
    "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96230315-fbef-4bc6-89d6-71b20b8e542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.randint(0, len(proverbs), 1)[0]\n",
    "query_line = proverbs[random_idx]\n",
    "\n",
    "\n",
    "random_idx = np.random.randint(0, len(proverbs), 1)[0]\n",
    "target_line = proverbs[random_idx]\n",
    "\n",
    "\n",
    "query_line, target_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23eed9-401f-42c0-9abc-54ad0f1ce824",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cf541-c8ff-473a-9779-799b8eb78d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokens = tokenizer.encode(query_line, add_special_tokens=False)\n",
    "t_tokens = tokenizer.encode(target_line, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe70f66-3dc4-475a-b753-5b809e0e9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokens, t_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbb72b-a7e1-48e5-8c41-217fdde83eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token = q_tokens[:len(q_tokens) // 2] + t_tokens[len(t_tokens) // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4eb77-d8a5-421c-99e0-071d3b6bbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line = tokenizer.decode(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6dc85-3ee5-4714-a7ea-ea9c326cf014",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae50-47f7-4ed2-8978-13076a7b5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_proverbs(db_size):\n",
    "    lines = []\n",
    "    \n",
    "    for _ in range(db_size):\n",
    "        random_idx = np.random.randint(0, len(proverbs), 1)[0]\n",
    "        query_line = proverbs[random_idx]\n",
    "\n",
    "        random_idx = np.random.randint(0, len(proverbs), 1)[0]\n",
    "        target_line = proverbs[random_idx]\n",
    "        \n",
    "        q_tokens = tokenizer.encode(query_line, add_special_tokens=False)\n",
    "        t_tokens = tokenizer.encode(target_line, add_special_tokens=False)\n",
    "        \n",
    "        new_token = q_tokens[:len(q_tokens) // 2] + t_tokens[len(t_tokens) // 2:]\n",
    "        new_line = tokenizer.decode(new_token)\n",
    "        \n",
    "        lines.append(new_line)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56449a1b-5618-4b4e-89dc-f581e6488326",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "\n",
    "fake_proverbs = generate_fake_proverbs(data_size // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875f638-b434-4201-b11d-e979f5599280",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idxs = np.random.randint(0, len(proverbs), data_size // 2)\n",
    "\n",
    "train_data = np.hstack([np.array(proverbs)[random_idxs], fake_proverbs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797c24b-5c04-4535-82f5-a8eb5f4667ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0db61-59a9-4de0-9a8a-ecff2544252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenizer.encode(x, add_special_tokens=True) for x in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289bf90-4a44-4dcd-b3fa-32aa4d452348",
   "metadata": {},
   "source": [
    "\n",
    "### Padding\n",
    "After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c48f74-56c2-47b5-bc3d-71fdad4e423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(x) for x in tokenized])\n",
    "print(max_len)\n",
    "\n",
    "padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6f151-dc27-4e39-af25-d8efd6e18cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947787e-8f7a-4f3a-a598-ecc1bbd4f1d2",
   "metadata": {},
   "source": [
    "### Masking\n",
    "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98434b71-c1de-408a-b5d2-4882e77e213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b06b66-00d7-4bee-b170-dd5b59e9725b",
   "metadata": {},
   "source": [
    "### Step #1: And Now, Deep Learning!\n",
    "Now that we have our model and inputs ready, let's run our model!\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n",
    "\n",
    "The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe44202-32ea-443e-860a-70c39f47b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc69eaf-b437-449d-b12f-5fa134cb3aaf",
   "metadata": {},
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
    "\n",
    "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db531d-5935-45fc-91d5-00a587429960",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape\n",
    "\n",
    "last_hidden_states[0].shape\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111c6c7-3cfd-4974-9c1e-30df6cec0c51",
   "metadata": {},
   "source": [
    "The labels indicating which sentence is positive and negative now go into the `labels` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e32ded-9090-42c7-9c3f-12d2c2266d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.ones(len(features))\n",
    "labels[len(features) // 2:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69cfb6-2466-4216-9268-fbcd4771a45e",
   "metadata": {},
   "source": [
    "### Step #3: Train/Test Split\n",
    "Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84017285-ad99-430f-b50c-0b13b3ed09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d0460-d0fb-46c6-b6be-26acf80fd756",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c4700-3ccb-488b-a794-d087ef652eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a281f-3969-4129-9da9-6c11d17e8526",
   "metadata": {},
   "source": [
    "### Estimate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e47b3-1046-4bc9-bb9a-b2511088f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "proba = lr_clf.predict_proba(train_features)[:, 1]\n",
    "auc = roc_auc_score(train_labels, proba)\n",
    "plt.plot(*roc_curve(train_labels, proba)[:2], label=f'train AUC={auc:.4f}')\n",
    "\n",
    "proba = lr_clf.predict_proba(test_features)[:, 1]\n",
    "auc = roc_auc_score(test_labels, proba)\n",
    "plt.plot(*roc_curve(test_labels, proba)[:2], label=f'test AUC={auc:.4f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba13ee-bf28-4e19-8d67-a260fe8574eb",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ebf67-52ec-4505-a5d6-c864d87db90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_phrase = 'Волк не птица - яйца не отложит!'\n",
    "token = tokenizer(text_phrase, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f42a3-7557-4634-a1bf-38f3ff14f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = last_hidden_states[0][0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d5401-320a-4dc9-af56-bc18bab12275",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.predict_proba([feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa15a3-4890-45d5-a358-de2a122a8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.predict([feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4d196-dcd1-440a-ab38-f1ecfb973392",
   "metadata": {},
   "source": [
    "## Part 2. Use all data and explore results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68037484-8764-4480-b410-832aa33d066a",
   "metadata": {},
   "source": [
    "Generate more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d8279-ba1e-4ee8-bf9f-e8cc81f9c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8dc96-938d-4bf5-9b6b-da6d73551a1b",
   "metadata": {},
   "source": [
    "Itarate by batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59233-72c1-40d1-95c9-19bd261ec621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ee32a-2297-4aaf-8a3e-c4b6c4de1bcb",
   "metadata": {},
   "source": [
    "#### Build sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6ea44-c7a6-4a26-b583-cdbb2e541566",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc46db-ffce-4dd3-a8dd-ee9791ef9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15124c-c707-478a-a3e0-9124bc0f6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390018b-a0b5-4bcb-9081-03274662315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "proba = lr_clf.predict_proba(train_features)[:, 1]\n",
    "auc = roc_auc_score(train_labels, proba)\n",
    "plt.plot(*roc_curve(train_labels, proba)[:2], label=f'train AUC={auc:.4f}')\n",
    "\n",
    "proba = lr_clf.predict_proba(test_features)[:, 1]\n",
    "auc = roc_auc_score(test_labels, proba)\n",
    "plt.plot(*roc_curve(test_labels, proba)[:2], label=f'test AUC={auc:.4f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfac5f7-03da-437a-85f6-b4a571da5b47",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Explore results\n",
    "\n",
    "1. Get best chery-picks from fake proverbs\n",
    "2. Create your own proverb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00089c34-eca6-430d-9761-ba8af859c354",
   "metadata": {},
   "source": [
    "### Single phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d72549-6053-430e-a191-ce6c0ae3461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "text_phrase = 'Волк не птица - в лес зайдет'\n",
    "token = tokenizer(text_phrase, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590db4e-04fe-4be8-b338-8c831481d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = last_hidden_states[0][0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542e4c0-bfc8-4e7a-8fa5-461bf663c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.predict_proba([feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502482d-aa27-4943-acb0-bcd964176b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.predict([feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550273e-69b7-4b6b-8a94-ea0c2756473f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chery-pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b334706-6d5e-4628-b72a-c17ffb005621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c98d0a-c5f3-4353-9ba8-345fc333ca8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
