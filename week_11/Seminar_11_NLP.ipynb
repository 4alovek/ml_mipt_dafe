{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Seminar_11_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvJKUUlNllnv"
      },
      "source": [
        "# Предобработка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4DSJfdh0DzL"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD0krM4r0DzN"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugnQIr010DzO"
      },
      "source": [
        "# 1 Стемминг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bejrQ2l0DzQ",
        "outputId": "6ae4edb9-32a6-4476-eb45-b85a35e3fac3"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeixON-M0DzO"
      },
      "source": [
        "words= [\"learn\", \"learning\", \"learned\", \"learns\"]\n",
        "ps =  PorterStemmer()\n",
        "for w in words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(rootWord)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NYm1hdI0DzP"
      },
      "source": [
        "sentence=\"Dear students, You should try very hard to master machine learning!\"\n",
        "words = nltk.word_tokenize(sentence)\n",
        "ps = PorterStemmer()\n",
        "word_list = []\n",
        "for w in words:\n",
        "    rootWord=ps.stem(w)\n",
        "    word_list.append(rootWord)\n",
        "print(\"Список слов из предложения:\\n\", word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M897x-iK0DzP"
      },
      "source": [
        "# 2 Лематтизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bQc4whinQJC"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5BJFjnQ0DzQ"
      },
      "source": [
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatize Single Word\n",
        "print(lemmatizer.lemmatize(\"classes\"))\n",
        "print(lemmatizer.lemmatize(\"women\"))\n",
        "print(lemmatizer.lemmatize(\"crying\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQcV0pBv0DzR"
      },
      "source": [
        "# Напишем какое-нибудь предложение\n",
        "sentence = \"Bad students were expelled from the institute\"\n",
        "\n",
        "# Сделаем его токенизацию, то есть разобьем на слова\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "print(\"Список слов из предложения:\\n\", word_list)\n",
        "\n",
        "# Сделаем леммитизацию каждого слова\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "print(\"Предложение из лемматизированных слов:\\n\", lemmatized_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akXmQtSf0DzR"
      },
      "source": [
        "# Иногда одно и то же слово может иметь несколько лемм в зависимости от значения/части речи/контекста\n",
        "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
        "print(lemmatizer.lemmatize(\"stripes\", 'n')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно получить [теги](https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk) токена предобученным алгоритмом."
      ],
      "metadata": {
        "id": "b9XXT47b_45h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKDQAcqj0DzS"
      },
      "source": [
        "print(nltk.pos_tag(['women']))\n",
        "print(nltk.pos_tag(nltk.word_tokenize(sentence)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af-LdxLW0DzS"
      },
      "source": [
        "# Lemmatize with POS Tag\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    # print(tag, nltk.pos_tag([word]))\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# 1. Init Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 2. Lemmatize Single Word with the appropriate POS tag\n",
        "word = 'women'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
        "\n",
        "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zmE1ftq0DzS"
      },
      "source": [
        "# 4 Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yQmvZdMzs8C"
      },
      "source": [
        "## 4.1 Via python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_T93Dgl7gVX"
      },
      "source": [
        "#Write the sentences in the corpus,in our case, just two \n",
        "string1=\"Welcome to Great Learning , Now start learning\"\n",
        "string2=\"Learning is a good practice\"\n",
        "\n",
        "#create a list of stopwords.You can import stopwords from nltk too\n",
        "stopwords=\n",
        "\n",
        "#list of special characters.You can use regular expressions too\n",
        "special_char=\n",
        "\n",
        "#convert them to lower case\n",
        "# Ваш код здесь\n",
        "string1=\n",
        "string2=\n",
        "\n",
        "#split the sentences into tokens\n",
        "# Ваш код здесь\n",
        "tokens1=\n",
        "tokens2=\n",
        "print(tokens1)\n",
        "print(tokens2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re_EYZMfoRps"
      },
      "source": [
        "def unique(sequence):\n",
        "    '''This functions returns a list in which the order remains \n",
        "    same and no item repeats.Using the set() function does not \n",
        "    preserve the original ordering'''\n",
        "    \n",
        "    return # Ваш код здесь"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OjnTYJY7gaj"
      },
      "source": [
        "#create a vocabulary list\n",
        "vocab=unique(tokens1+tokens2) # Ваш код здесь\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKpT8ZM47krL"
      },
      "source": [
        "#filter the vocabulary list\n",
        "\n",
        "filtered_vocab=[]\n",
        "for w in vocab: \n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "print(filtered_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ0sWCd1oQL5"
      },
      "source": [
        "def vectorize(tokens):\n",
        "    ''' This function takes list of words in a sentence as input \n",
        "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
        "    word is not present in tokens and count of token if present.'''\n",
        "    vector=[]\n",
        "    \n",
        "    # Ваш код здесь\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqxDa5i10DzT"
      },
      "source": [
        "#convert sentences into vectors\n",
        "vector1=vectorize(tokens1)\n",
        "print(vector1)\n",
        "vector2=vectorize(tokens2)\n",
        "print(vector2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJkijEN0zzQa"
      },
      "source": [
        "## 4.2 Via sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8_xs-Wd0DzT"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        " \n",
        "sentence_1=\"Welcome to Great Learning , Now start learning\"\n",
        "sentence_2=\"Learning is a good practice\"\n",
        "  \n",
        "CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
        "#transform\n",
        "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
        " \n",
        "#create dataframe\n",
        "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
        "print(cv_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvIgMKQ_-0gs"
      },
      "source": [
        "# 5 N-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ZpN6wr-x7R"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        " \n",
        "sentence_1=\"This is a good job.I will not miss it for anything\"\n",
        "sentence_2=\"This is not good at all\"\n",
        "  \n",
        "CountVec = CountVectorizer(ngram_range=(1,2))\n",
        "#transform\n",
        "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
        " \n",
        "#create dataframe\n",
        "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
        "print(cv_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UcGOjhCyySP"
      },
      "source": [
        "# 6 TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BJ64wQ6_6nR"
      },
      "source": [
        "$TF(i,j)= \\frac{n(i,j)}{\\sum{ n(i,j)}}$\n",
        "\n",
        "$IDF=\\log(N/dN)$\n",
        "\n",
        "$TF-IDF=TF*IDF$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UbtxWD10DzT"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        " \n",
        "sentence_1=\"This is a good job.I will not miss it for anything\"\n",
        "sentence_2=\"This is not good at all\"\n",
        " \n",
        "#define tf-idf\n",
        "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
        "                        smooth_idf=False,  \n",
        "                        ngram_range=(1,1))\n",
        "#transform\n",
        "tf_idf_data = tf_idf_vec.fit_transform([sentence_1,sentence_2])\n",
        " \n",
        "#create dataframe\n",
        "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
        "print(tf_idf_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_1T5LEgH2lL"
      },
      "source": [
        "# 7 Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DikHsCQpH2lL"
      },
      "source": [
        "Установим полезную утилиту Python для парсинга веб-страниц. Она поможет нам достать статью Векипедии, с которой мы будем работать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu5x1nF9H2lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389a67ab-ab8e-44a5-f1c5-f334f1a57cb1"
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZbMOUaRH2lM"
      },
      "source": [
        "И еще установим библиотеку для синтаксического анализа HTML страниц."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oooq0-JeH2lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af1e75e-5165-4a3f-cd02-d7964cdaca18"
      },
      "source": [
        "!pip install lxml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erq1W2f-H2lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1a08ac-9e12-4f4c-cc4a-d1eb6ddeff6e"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMF6_mmQH2lM"
      },
      "source": [
        "Статья, с которой будем работать, конечно, про искусственный интеллект:) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQGApu7yH2lN"
      },
      "source": [
        "# Загрузим статью по ссылке\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
        "\n",
        "# Сдесь статья еще в куче с HTML кодами\n",
        "article = scraped_data.read()\n",
        "\n",
        "# А теперь статья приняла нормальный вид\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "# Википедия хранит текстовое содержимое статьи внутри p-тегов. Вытащим текст\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "article_text = \"\"\n",
        "for p in paragraphs:\n",
        "    article_text += p.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Baae6F3uH2lN"
      },
      "source": [
        "Убедимся, что текст без HTML кодов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AztMw8FjH2lN"
      },
      "source": [
        "article_text[0:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LazygxJdH2lN"
      },
      "source": [
        "Супер!\n",
        "\n",
        "Дальше сделаем [предобработку текста](https://tproger.ru/translations/regular-expression-python/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X23MFSiFH2lN"
      },
      "source": [
        "# Удаляем прописные буквы\n",
        "processed_article = article_text.lower()\n",
        "\n",
        "# Удаляем все цифры, специальные символы\n",
        "processed_article = \n",
        "\n",
        "# Удаляем лишние пробелы из текста\n",
        "processed_article = \n",
        "\n",
        "# Делим статью на предложения\n",
        "# Хоть оно и получается одно, но это нужно, чтобы word_tokenize воспринимал слова как слова, а не буквы как слова\n",
        "all_sentences = nltk.sent_tokenize(processed_article)\n",
        "\n",
        "# Теперь разделим на слова\n",
        "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
        "\n",
        "# Удаляем слова, не дающие никакой информации (предлоги, артикли и тд)\n",
        "from nltk.corpus import stopwords\n",
        "for i in range(len(all_words)):\n",
        "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dww_bmXH2lO"
      },
      "source": [
        "Посмотрим первые несколько слов нашего \"чистого\" списка."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TIs0kWyH2lO"
      },
      "source": [
        "all_words[0][0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB1ew-36H2lO"
      },
      "source": [
        "Теперь нужно установить библиотеку обработки естественного языка. С её помощью можно обрабатывать тексты, работать с векторными моделями слов, такими как Word2Vec (в нашем случае)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvoCYYuUH2lO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dea4df-56a0-4146-e512-22210828037f"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu6LBPuCH2lO"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJpLA2RTH2lP"
      },
      "source": [
        "Создадим модель Word2Vec с использованием статьи в Википедии, которую мы скопировали."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0PiAc6JH2lP"
      },
      "source": [
        "min_count=2 показывает, что в модель Word2Vec будут входить только те слова, которые встречаются в корпусе как минимум 2 раза."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtRqlF2aH2lP"
      },
      "source": [
        "word2vec = Word2Vec(all_words, min_count=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pCcbkABH2lP"
      },
      "source": [
        "Посмотри уникальные слова, которые в статье встреачются как минимум дважды."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsBO4V5YH2lP"
      },
      "source": [
        "Небольшое пояснение:\n",
        "\n",
        "Вы можете увидеть радом со словом что-то подобное(<gensim.models.keyedvectors.Vocab object at 0x00000282FDC67100>). Структура KeyedVectors представляет собой отображение между ключами и векторным представлением слова. Каждый вектор идентифицируется своим ключом поиска, чаще всего коротким строковым токеном. Это выглядит как {str => 1D numpy array}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aN6yfSLvH2lP"
      },
      "source": [
        "# model.wv - свойство модели, в котором хранятся отдельные ключевые векторы\n",
        "vocab = word2vec.wv.vocab\n",
        "print(vocab)\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_g9-9CH2lQ"
      },
      "source": [
        "Посмотрим векторное представление какого-нибудь слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loc9wqlyH2lQ"
      },
      "source": [
        "v1 = word2vec.wv['artificial']\n",
        "print(v1)\n",
        "print(len(v1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ny2VOsoH2lQ"
      },
      "source": [
        "И найдем все похожие слова. Так же увидим их индексы подобия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hKnX-ASH2lQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e93e5261-42b8-465e-8114-32bbdebc776f"
      },
      "source": [
        "sim_words = word2vec.wv.most_similar('intelligence')\n",
        "sim_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('networks', 0.43329304456710815),\n",
              " ('ai', 0.3645763695240021),\n",
              " ('mathematics', 0.36402207612991333),\n",
              " ('processes', 0.36377769708633423),\n",
              " ('several', 0.353674054145813),\n",
              " ('levels', 0.34761884808540344),\n",
              " ('research', 0.342585027217865),\n",
              " ('human', 0.3377702832221985),\n",
              " ('learning', 0.3350643515586853),\n",
              " ('many', 0.3330388069152832)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMQd2kBl0DLe"
      },
      "source": [
        "# 8 Практика. Генерация текста"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laIXGCqnG1Vp",
        "outputId": "366f11f0-e137-4660-a3cf-9a01c1d6c600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAlbEGXN0DLX"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moT4y7wD0DLf"
      },
      "source": [
        "## 8.1 Подготовка текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIOfWhRVmLtZ"
      },
      "source": [
        "### 8.1.1 Выбор корпуса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P060NMLb0DLf"
      },
      "source": [
        "Сегодня мы разберем принципы работы с рекурентными сетями с использованием фреймфорка Keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmy2qt7e0DzU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePBD7g9H0DLf"
      },
      "source": [
        "directory = './gdrive/My Drive/Colab Notebooks/ML/семестр2/Семинар 11 - Basic NLP/txt/'\n",
        "files = os.listdir(directory)\n",
        "files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef862xTT0DLh"
      },
      "source": [
        "corpus = directory + files[0]\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4KN6npn0DLh"
      },
      "source": [
        "### 8.1.2 Предобработка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgy0iJ680DLh"
      },
      "source": [
        "with io.open(corpus, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('Длина корпуса:', len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOE5j5Q20DLi"
      },
      "source": [
        "Посмотрим на часть текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHzfSfLf0DLi"
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNSqAkjv0DLi"
      },
      "source": [
        "Сформируйте последовательности длинны `maxlen` со сдвигом `step`, следующим образом: \n",
        "- В переменную `sentences` включите последовательность\n",
        "- `next_chars` список символов который является следующим для указанной последовательности"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39fgvTgx0DLj"
      },
      "source": [
        "# Ваш код здесь"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuUn75tG0DLj"
      },
      "source": [
        "print('Колличество послеовательностей:', len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmsgAi590DLj"
      },
      "source": [
        "Посмотрим на случайную пару: Последовательность/ответ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0kB_r8N0DLj"
      },
      "source": [
        "check_ran = np.random.randint(0,len(sentences))\n",
        "print('После строки:')\n",
        "print(sentences[check_ran])\n",
        "print('')\n",
        "print('Ожидаем: ',next_chars[check_ran])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl0A7kw70DLk"
      },
      "source": [
        "### 8.1.3 Векторизация\n",
        "Далее, нам необходимо векторизвать каждую букву, для этого воспользуемся обычным Bag of chars:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwnvt8xk0DLk"
      },
      "source": [
        "chars = sorted(list(set(text))) #Составив список всех символов\n",
        "print('Всего символов:', len(chars))\n",
        "# Составьте два словаря\n",
        "# Первый: список букв:номера индексов\n",
        "# Второй: номера индексов:список букв\n",
        "\n",
        "# Ваш код здесь"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J_ZG6hL0DLk"
      },
      "source": [
        "indexes_chars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKQLmjWO0DLl"
      },
      "source": [
        "Составьте обучающую выборку, где:  \n",
        "- x - это закодированные последовательности, размерностью: (количество объектов, длинна последовательности, колличество уникальных букв), таким образом каждой букве в последовательности будет соответствовать вектор\n",
        "- y - это закодированные ответы, размерностью: (количество объектов, колличество уникальных букв)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW-ypFES0DLl"
      },
      "source": [
        "print('Векторизация...', )\n",
        "x = np.zeros(, dtype=np.integer)\n",
        "y = np.zeros(, dtype=np.integer)\n",
        "\n",
        "\n",
        "# Ваш код здесь\n",
        "for i, sec in enumerate(sentences):\n",
        "    for j, token in enumerate(sec):\n",
        "      # Ваш код здесь\n",
        "\n",
        "\n",
        "print('... Готово')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szacZvZQ0DLl"
      },
      "source": [
        " Проверим, размерность:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6_gNH8_0DLl"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufvlSbP30DLl"
      },
      "source": [
        "## 8.2 Подготовка к генерации текста\n",
        "Создадим функцию `sample`, которая будет выбирать следующую букву на основании предсказания и температуры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEiXtlor0DLm"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    # Ваш код здесь\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBx85Rmt0DLm"
      },
      "source": [
        "> Temperature. We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:\n",
        ">> “is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”\n",
        "\n",
        "> looks like we’ve reached an infinite loop about startups.\n",
        "\n",
        "Источник: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "dAuPeex30DLm"
      },
      "source": [
        "## 8.3 Обучение моделей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Jc299p0DLm"
      },
      "source": [
        "from keras.layers import SimpleRNN, GRU, LSTM "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsKhobHv0DLn"
      },
      "source": [
        "### 8.3.1 VanilaRNN (повторение)\n",
        "Создадим простую RNN с одним слоем из 128 нейронов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqS5YF8x0DLn"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm2mYOvO0DLn"
      },
      "source": [
        "model.fit(x, y, batch_size=256,\n",
        "          epochs=7, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMTkgrEg0DLn"
      },
      "source": [
        "### 8.3.2 Сгенерируем текст"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtRob0HcGjXK"
      },
      "source": [
        "def generating_text(diversity, model):\n",
        "    print()\n",
        "    \n",
        "    generated = ''\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    print(generated)\n",
        "    print('...Генерация текста. Температура: ', diversity)\n",
        "    sys.stdout.write(generated)\n",
        "    for i in range(100):\n",
        "        x_pred = np.zeros()\n",
        "        # Ваш код здесь\n",
        "\n",
        "        preds =  #Вектор вероятностей, полученный с помощью модели\n",
        "        next_index = # Выбираем индекс\n",
        "        next_char =  #Выбираем символ из словаря \n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiVekhHV0DLn"
      },
      "source": [
        "generating_text(.5, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7aWtN8U0DLo"
      },
      "source": [
        "Поэкспериментируйте с параметром `diversity` и числом слоев и посмотрите как изменяется генерируемый текст"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoTt_zIw0DLp"
      },
      "source": [
        "# В следующей серии\n",
        "Больше экспериментов\n",
        "Попробуйте различные варианты сетей: GRU и LSTM. Сравните результаты и выберите удачные примеры генерации текста. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgfZ1yMV-FUX"
      },
      "source": [
        "# Ссылки\n",
        "- [Тэги](https://medium.com/@muddaprince456/categorizing-and-pos-tagging-with-nltk-python-28f2bc9312c3)\n",
        "- [Больше про ленгвистическую предобработку текстов](https://www.mygreatlearning.com/blog/natural-language-processing-tutorial/)\n",
        "- [Статья про TF-IDF](https://m.habr.com/ru/company/Voximplant/blog/446738/)"
      ]
    }
  ]
}