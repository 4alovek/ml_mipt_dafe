{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKuuu7rcyYjD"
   },
   "source": [
    "# Семинар № 7 - Задача распознавания# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45dWzOZvyYjG"
   },
   "source": [
    "### 1.EDA\n",
    "### 2. Writing a custom dataset for our work\n",
    "### 3. Define the model\n",
    "### 4. Finetuning from a pretrained model\n",
    "### 5. Prediction\n",
    "### 6. End Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P28KqUbyYjG"
   },
   "source": [
    "# 1.EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sc0D9M0ryYjH"
   },
   "source": [
    "### Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "executionInfo": {
     "elapsed": 3122,
     "status": "ok",
     "timestamp": 1647955869114,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "QWMClV5syYjH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1647955869115,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "MbWon6wuyYjI"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1647955869115,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "p9CtjYqhyYjI"
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcBBnt23yYjK"
   },
   "source": [
    "##  Load train and test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1647955278314,
     "user": {
      "displayName": "Никита Гришин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5pEOVC4RUh6bPjjTIzOxO5ZeGRC7LTQygO1Uffg=s64",
      "userId": "00970267534020248746"
     },
     "user_tz": -180
    },
    "id": "iU3USv5yyYjK"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"global-wheat-detection/train.csv\")\n",
    "submit = pd.read_csv(\"global-wheat-detection/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA7BQqlPyYjL"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FK7HRmEiyYjL"
   },
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns=['width','height','source']) #Drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2jDQspHyYjL"
   },
   "outputs": [],
   "source": [
    "train_df['image_id'].nunique() # There are total 3373 unique image in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfnsC8GhyYjL"
   },
   "outputs": [],
   "source": [
    "(train_df['image_id'].value_counts()).max()  # maximum number of boxes in a single image are 116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_s9VcvXyYjM"
   },
   "outputs": [],
   "source": [
    "(train_df['image_id'].value_counts()). min() # Minimum number of box in a single image is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIcv7kusyYjM"
   },
   "source": [
    "### Splitting the dimension of box in the formate [xmin, ymin, w, h]\n",
    "#### Latter on we will convert the deminsion of box into [xmin, ymin, xmax, ymax] in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHxTWVD1yYjM"
   },
   "outputs": [],
   "source": [
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGG57mpayYjM"
   },
   "outputs": [],
   "source": [
    "train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x))) ##Lets convert the Box in \n",
    "train_df['x'] = train_df['x'].astype(np.float32)                                        #in our desired formate    \n",
    "train_df['y'] = train_df['y'].astype(np.float32)\n",
    "train_df['w'] = train_df['w'].astype(np.float32)\n",
    "train_df['h'] = train_df['h'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmjpq1S5yYjN"
   },
   "outputs": [],
   "source": [
    "train_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4j2sWccyYjN"
   },
   "outputs": [],
   "source": [
    "submit[['x', 'y', 'w', 'h']] = np.stack(submit['PredictionString'].apply(lambda x: [0, 0, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxYyUbmAyYjN"
   },
   "outputs": [],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YK6C_9_VyYjN"
   },
   "source": [
    "### Splitting the data into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnzHvswEyYjN"
   },
   "outputs": [],
   "source": [
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMbfN4cwyYjO"
   },
   "source": [
    "# 2.Writing a custom dataset for our work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVNo9HXlyYjO"
   },
   "source": [
    "### 2.1 Writing a custom dataset for train and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k--f6bacyYjO"
   },
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None,train=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.train=train\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        records = self.df[self.df['image_id'] == image_id]   \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        # throw to xyxy\n",
    "        pass\n",
    "\n",
    "        # there is only one class\n",
    "        labels = None\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "        if self.transforms:\n",
    "            pass\n",
    "        \n",
    "        return image, target, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnXkVw-pyYjO"
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            None,\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YKLzuXcyYjO"
   },
   "outputs": [],
   "source": [
    "train_dir = 'global-wheat-detection/train'\n",
    "test_dir = 'global-wheat-detection/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-wFs3h8yYjO"
   },
   "outputs": [],
   "source": [
    "class Averager:      ##Return the average loss \n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = WheatDataset(train_df, train_dir, get_train_transforms(), True)\n",
    "valid_dataset = WheatDataset(valid_df, train_dir, get_valid_transforms(), True)\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMA4SVO8yYjP"
   },
   "source": [
    "### Lets visualize some of the images with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zr8I5Lr1yYjP"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPr9u799yYjP"
   },
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "idx = 2\n",
    "boxes = targets[idx]['boxes'].cpu().numpy().astype(int)\n",
    "sample = images[idx].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    sample = cv2.rectangle(sample.copy(), (box[0], box[1]), (box[2], box[3]), (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaov3rHtyYjP"
   },
   "source": [
    "# 3.Finetuuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHrnWAnbyYjP"
   },
   "source": [
    "### Defining the model\n",
    "\n",
    "Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Let’s explain how this architecture works, Faster RCNN is composed from 3 parts\n",
    "\n",
    "1. Part 1 : Convolution layers : A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n",
    "\n",
    "2. Part 2 : Region Proposel Network (RPN) : RPN is small neural network sliding on the last feature map of the convolution layers and predict wether there is an object or not and also predict the bounding box of those objects.\n",
    "\n",
    "3. Part 3 : Classes and Bounding Boxes prediction : Now we use another Fully connected neural networks that takes as an inpt the regions proposed by the RPN and predict object class ( classification) and Bounding boxes (Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmAIl3dFyYjR"
   },
   "outputs": [],
   "source": [
    "# load a model; pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, pretrained_backbone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8laxHogyYjS"
   },
   "outputs": [],
   "source": [
    "num_classes = 2  # 1 class (wheat) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qwd6k79syYjS"
   },
   "source": [
    "### Lets train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v_1QZ9zyYjS"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in tqdm(train_data_loader):\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)   ##Return the loss\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values()).type(torch.float32)\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)  #Average out the loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knPz1brkyYjS"
   },
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK48k_TAyYjS"
   },
   "source": [
    "### Lets load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZMHkGrOyYjS"
   },
   "outputs": [],
   "source": [
    "test_dataset = WheatDataset(submit, test_dir, get_valid_transforms(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olAWpoVUyYjS"
   },
   "outputs": [],
   "source": [
    "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  ##Test dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyhG2ZnkyYjT"
   },
   "source": [
    "### Set the threshold value for predicting bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxeVaAaxyYjT"
   },
   "outputs": [],
   "source": [
    "detection_threshold = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI0lM2sPyYjT"
   },
   "outputs": [],
   "source": [
    "## Lets make the prediction\n",
    "results = []\n",
    "model.eval()\n",
    "\n",
    "for images, _, image_ids in test_data_loader:    \n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()    ##Formate of the output's box is [Xmin,Ymin,Xmax,Ymax]\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32) #Compare the score of output with the threshold and\n",
    "        scores = scores[scores >= detection_threshold]                    #slelect only those boxes whose score is greater\n",
    "                                                                          # than threshold value\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]         #Convert the box formate to [Xmin,Ymin,W,H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjRvZpoQyYjT"
   },
   "outputs": [],
   "source": [
    "sample = images[1].permute(1, 2, 0).cpu().numpy()\n",
    "boxes = outputs[1]['boxes'].data.cpu().numpy()\n",
    "scores = outputs[1]['scores'].data.cpu().numpy()\n",
    "\n",
    "boxes = boxes[scores >= detection_threshold].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJYMAG8YyYjT"
   },
   "source": [
    "### Lets plot some of our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_p1CMe2yYjT"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    sample = cv2.rectangle(sample.copy(),\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 2)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylCLRsbhyYjT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OIcv7kusyYjM",
    "YK6C_9_VyYjN",
    "UVNo9HXlyYjO",
    "ZMA4SVO8yYjP",
    "VHrnWAnbyYjP",
    "Qwd6k79syYjS",
    "GK48k_TAyYjS",
    "NyhG2ZnkyYjT",
    "aJYMAG8YyYjT"
   ],
   "name": "seminar_07_solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
