{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"seminar_09_value_and_policy_iteration_Solved.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"VwT1_i5urB-S"},"source":["# Value Iteration. Policy Iteration."]},{"cell_type":"code","source":["import gym\n","import os\n","import numpy as np\n","import itertools\n","from collections import defaultdict\n","from gym.envs import toy_text\n","import matplotlib.pyplot as plt\n","from IPython.display import Image\n","from IPython import display\n","%matplotlib inline\n","gym.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5tThXctua8EH","executionInfo":{"status":"ok","timestamp":1649176359980,"user_tz":-180,"elapsed":872,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"408e151f-d988-4537-cb49-b78fd90dbbc2"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.17.3'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"wkzMNoirrB-c"},"source":["## Value Iteration"]},{"cell_type":"markdown","metadata":{"id":"CeS58CcZrB-c"},"source":["На лекции мы рассмотрели, как мы можем выучить оптимальную политику, используя алгоритм Value Iteration, если нам известна динамика среды, а также если пространства состояний и действий не большие и дискретные."]},{"cell_type":"markdown","metadata":{"id":"j8V198ZWrB-c"},"source":["Попробуем выучить оптимальную политику в среде <a href=https://gym.openai.com/envs/FrozenLake-v0>FrozenLake-v0</a>. Это простая среда с маленькими пространствами состояний и действий, а также с известной динамикой."]},{"cell_type":"markdown","metadata":{"id":"R1RDsVzIrB-c"},"source":["Создадим среду и выведем её описание."]},{"cell_type":"code","metadata":{"id":"9dGUx3aVrB-c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649176359981,"user_tz":-180,"elapsed":13,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"c74fc48d-9bf9-49ce-b94b-c34e0695d7f3"},"source":["env = gym.make('FrozenLake-v0')\n","env.reset()\n","\n","print(\"Observation space:\", env.observation_space)\n","print(\"Action space:\", env.action_space)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Observation space: Discrete(16)\n","Action space: Discrete(4)\n"]}]},{"cell_type":"code","metadata":{"id":"jWZQhZXMrB-d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649176359982,"user_tz":-180,"elapsed":11,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"cdae6ad8-e064-444e-9187-3da69c19ad42"},"source":["print(env.env.__doc__)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Winter is here. You and your friends were tossing around a frisbee at the\n","    park when you made a wild throw that left the frisbee out in the middle of\n","    the lake. The water is mostly frozen, but there are a few holes where the\n","    ice has melted. If you step into one of those holes, you'll fall into the\n","    freezing water. At this time, there's an international frisbee shortage, so\n","    it's absolutely imperative that you navigate across the lake and retrieve\n","    the disc. However, the ice is slippery, so you won't always move in the\n","    direction you intend.\n","    The surface is described using a grid like the following\n","\n","        SFFF\n","        FHFH\n","        FFFH\n","        HFFG\n","\n","    S : starting point, safe\n","    F : frozen surface, safe\n","    H : hole, fall to your doom\n","    G : goal, where the frisbee is located\n","\n","    The episode ends when you reach the goal or fall in a hole.\n","    You receive a reward of 1 if you reach the goal, and zero otherwise.\n","    \n"]}]},{"cell_type":"markdown","metadata":{"id":"jeE7E_RrrB-d"},"source":["Как видно среда представляет собой поле 4 на 4, по которому нужно добраться от начала (клетка *S*) до цели (клетка *G*). При этом среда является недетерменированный - с определенной вероятностью при совершения действия агент подскользнется и попадет не в ту клетку, в которую направлялся. Клетка *H* обозначает прорубь. Игра закначивается, когда агент попадает в клетку *G* или в клету *H*. Если агент проваливается в прорубь, то он получает награду *0*, если достигает клетки цели *1*. "]},{"cell_type":"markdown","metadata":{"id":"LKfH731DrB-d"},"source":["Посмотрим, сколько в среднем очков награды за 100 эпизодов получит наш агент, если будет выполнять случайные действия."]},{"cell_type":"code","metadata":{"id":"mebroA_frB-d","executionInfo":{"status":"ok","timestamp":1649176359983,"user_tz":-180,"elapsed":9,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}}},"source":["env.seed(42);"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"fandrCwXrB-d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649176360669,"user_tz":-180,"elapsed":694,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"39f3152c-f1e9-4f71-bac9-1e2b96d0900f"},"source":["total_reward = []\n","for episode in range(100):\n","    episode_reward = 0\n","    observation = env.reset()\n","    for t in range(100):\n","        # env.render()\n","        action = env.action_space.sample()\n","        observation, reward, done, _ = env.step(action)\n","        episode_reward += reward\n","        if done:\n","            print(\"Episode {} finished after {} timesteps\".format(episode+1, t+1))\n","            break\n","    total_reward.append(episode_reward)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 finished after 16 timesteps\n","Episode 2 finished after 7 timesteps\n","Episode 3 finished after 7 timesteps\n","Episode 4 finished after 2 timesteps\n","Episode 5 finished after 10 timesteps\n","Episode 6 finished after 7 timesteps\n","Episode 7 finished after 15 timesteps\n","Episode 8 finished after 4 timesteps\n","Episode 9 finished after 8 timesteps\n","Episode 10 finished after 10 timesteps\n","Episode 11 finished after 2 timesteps\n","Episode 12 finished after 6 timesteps\n","Episode 13 finished after 4 timesteps\n","Episode 14 finished after 7 timesteps\n","Episode 15 finished after 4 timesteps\n","Episode 16 finished after 2 timesteps\n","Episode 17 finished after 7 timesteps\n","Episode 18 finished after 11 timesteps\n","Episode 19 finished after 2 timesteps\n","Episode 20 finished after 5 timesteps\n","Episode 21 finished after 6 timesteps\n","Episode 22 finished after 2 timesteps\n","Episode 23 finished after 3 timesteps\n","Episode 24 finished after 6 timesteps\n","Episode 25 finished after 4 timesteps\n","Episode 26 finished after 10 timesteps\n","Episode 27 finished after 9 timesteps\n","Episode 28 finished after 7 timesteps\n","Episode 29 finished after 8 timesteps\n","Episode 30 finished after 5 timesteps\n","Episode 31 finished after 10 timesteps\n","Episode 32 finished after 13 timesteps\n","Episode 33 finished after 4 timesteps\n","Episode 34 finished after 7 timesteps\n","Episode 35 finished after 5 timesteps\n","Episode 36 finished after 6 timesteps\n","Episode 37 finished after 3 timesteps\n","Episode 38 finished after 2 timesteps\n","Episode 39 finished after 4 timesteps\n","Episode 40 finished after 6 timesteps\n","Episode 41 finished after 5 timesteps\n","Episode 42 finished after 3 timesteps\n","Episode 43 finished after 19 timesteps\n","Episode 44 finished after 5 timesteps\n","Episode 45 finished after 11 timesteps\n","Episode 46 finished after 6 timesteps\n","Episode 47 finished after 5 timesteps\n","Episode 48 finished after 4 timesteps\n","Episode 49 finished after 8 timesteps\n","Episode 50 finished after 2 timesteps\n","Episode 51 finished after 4 timesteps\n","Episode 52 finished after 3 timesteps\n","Episode 53 finished after 4 timesteps\n","Episode 54 finished after 7 timesteps\n","Episode 55 finished after 2 timesteps\n","Episode 56 finished after 3 timesteps\n","Episode 57 finished after 5 timesteps\n","Episode 58 finished after 4 timesteps\n","Episode 59 finished after 23 timesteps\n","Episode 60 finished after 3 timesteps\n","Episode 61 finished after 3 timesteps\n","Episode 62 finished after 19 timesteps\n","Episode 63 finished after 5 timesteps\n","Episode 64 finished after 13 timesteps\n","Episode 65 finished after 7 timesteps\n","Episode 66 finished after 4 timesteps\n","Episode 67 finished after 3 timesteps\n","Episode 68 finished after 12 timesteps\n","Episode 69 finished after 7 timesteps\n","Episode 70 finished after 6 timesteps\n","Episode 71 finished after 10 timesteps\n","Episode 72 finished after 6 timesteps\n","Episode 73 finished after 3 timesteps\n","Episode 74 finished after 6 timesteps\n","Episode 75 finished after 8 timesteps\n","Episode 76 finished after 5 timesteps\n","Episode 77 finished after 5 timesteps\n","Episode 78 finished after 23 timesteps\n","Episode 79 finished after 5 timesteps\n","Episode 80 finished after 6 timesteps\n","Episode 81 finished after 10 timesteps\n","Episode 82 finished after 4 timesteps\n","Episode 83 finished after 3 timesteps\n","Episode 84 finished after 12 timesteps\n","Episode 85 finished after 10 timesteps\n","Episode 86 finished after 8 timesteps\n","Episode 87 finished after 19 timesteps\n","Episode 88 finished after 5 timesteps\n","Episode 89 finished after 3 timesteps\n","Episode 90 finished after 12 timesteps\n","Episode 91 finished after 8 timesteps\n","Episode 92 finished after 16 timesteps\n","Episode 93 finished after 3 timesteps\n","Episode 94 finished after 8 timesteps\n","Episode 95 finished after 8 timesteps\n","Episode 96 finished after 21 timesteps\n","Episode 97 finished after 18 timesteps\n","Episode 98 finished after 7 timesteps\n","Episode 99 finished after 4 timesteps\n","Episode 100 finished after 6 timesteps\n"]}]},{"cell_type":"code","metadata":{"id":"ohnx-1750U37","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1649176360670,"user_tz":-180,"elapsed":25,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"eb0254f5-aa9d-4625-de67-518076776bd6"},"source":["plt.plot(total_reward)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fba54725590>]"]},"metadata":{},"execution_count":6},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"FWJQNMjgrB-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649176360671,"user_tz":-180,"elapsed":17,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"9e16bdbf-13a7-484e-e327-6f947bddf57e"},"source":["print(np.mean(total_reward))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.01\n"]}]},{"cell_type":"markdown","metadata":{"id":"5Bkcsr7irB-e"},"source":["Как видим, только в 2 эпизодах из 100 агену удалось добраться до цели."]},{"cell_type":"markdown","metadata":{"id":"hP2WQJuDrB-e"},"source":["Из среды OpenAI Gym мы можем получить элементы MDP (Markov Decision Process)."]},{"cell_type":"markdown","metadata":{"id":"NE1bEiMvrB-e"},"source":["В env.env.P хранится двухуровневый словарь, в котором первый ключ является состояние, а второй - действием.\n","Клетки ассоциированыс индексами [0, 1, 2, ..., 15] слева направо и сверху вниз."]},{"cell_type":"markdown","metadata":{"id":"7fld4FxfrB-f"},"source":["Индексы действией [0, 1, 2, 3] соответствуют движению на Запад, Юг, Восток и Север.\n","env.env.P[state][action] возвращает лист кортежей (probability, nextstate, reward). Например, состояние 0 - это начальное состояние и информация о веротностях перехода для s=0 и a=0 содержит:"]},{"cell_type":"code","metadata":{"id":"ShWO5jHzrB-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649176431348,"user_tz":-180,"elapsed":437,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"e67a78cc-9026-4abe-b232-ad2493064350"},"source":["env.env.P[0][0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0.3333333333333333, 0, 0.0, False),\n"," (0.3333333333333333, 0, 0.0, False),\n"," (0.3333333333333333, 4, 0.0, False)]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"uRczJz33rB-f"},"source":["Другой пример - состояние 5 сооветсвует проруби, и все действия в данном состоянии приводят к тому же состоянию с вероятностью 1 и наградой 0."]},{"cell_type":"code","metadata":{"id":"p6v3d50WrB-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649176585700,"user_tz":-180,"elapsed":326,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"52fb1c91-5be9-44ac-faa3-639efaa638f1"},"source":["for i in range(4):\n","    print(\"P[5][%i] =\" % i, env.env.P[5][i])"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["P[5][0] = [(1.0, 5, 0, True)]\n","P[5][1] = [(1.0, 5, 0, True)]\n","P[5][2] = [(1.0, 5, 0, True)]\n","P[5][3] = [(1.0, 5, 0, True)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"ulL50EKqrB-g"},"source":["Вспомним, что из себя представляет алгоритм Value Iteration \n","![](https://drive.google.com/uc?export=view&id=1klIcBCbPCZMp5strcJFpkgGBePYdSNqr)"]},{"cell_type":"markdown","metadata":{"id":"PpOdbLucrB-g"},"source":["Задание считается решенным, если агент доходит до цели в среднем в 70% эпизодов."]},{"cell_type":"markdown","metadata":{"id":"FpqiI1NgrB-g"},"source":["Напишем несклько вспомогательных функций."]},{"cell_type":"markdown","metadata":{"id":"nhaaGfAtrB-g"},"source":["Запомним число состояний и действий в среде."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltyUvF-NrB-g","executionInfo":{"status":"ok","timestamp":1649176681160,"user_tz":-180,"elapsed":384,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"cb454460-8fc2-4263-8646-c79b4ad0bc18"},"source":["n_states = env.env.nS\n","n_actions = env.env.nA\n","print(\"Number of states: {}\".format(n_states))\n","print(\"Number of actions: {}\".format(n_actions))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of states: 16\n","Number of actions: 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"D17jl3KPrB-h"},"source":["Поскольку алгоритм Value Iteration возвращает нам оптимальную V-функцию, то нам необходимо извлекать из нее оптимальную политику (как указано в последней строке псевдокода алгоритма)."]},{"cell_type":"code","metadata":{"id":"xbflyrGbrB-h","executionInfo":{"status":"ok","timestamp":1649177654843,"user_tz":-180,"elapsed":612,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}}},"source":["def extract_policy(v, gamma = 1.0):\n","    policy = np.zeros(n_states)\n","    for state in range(n_states):\n","        q = np.zeros(n_actions)\n","        for action in range(n_actions):\n","            for next_sr in env.env.P[state][action]:\n","                probability, next_state, reward, _ = next_sr\n","                q[action] += (probability*(reward + gamma*v[next_state]))\n","        policy[state] = np.argmax(q)\n","    return policy"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGM8knFyrB-h"},"source":["Также напишем функцию для оценки нашей найденной политики."]},{"cell_type":"code","metadata":{"id":"APPFY7mjrB-i"},"source":["def evaluate_policy(env, policy, gamma=1.0, n=100):\n","    total_reward = []\n","    for episode in range(n):\n","        episode_reward = 0\n","        observation = env.reset()\n","        step = 0\n","        for _ in range(100):\n","            env.render()\n","            action = int(policy[observation])\n","            observation, reward, done, _ = env.step(action)\n","            episode_reward += gamma**step*reward\n","            step += 1\n","            if done:\n","                break\n","        total_reward.append(episode_reward)\n","    return np.mean(total_reward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcMS01htrB-i"},"source":["Нам остается написать основную функцию, которая вернет оптимальную V-функцию."]},{"cell_type":"code","metadata":{"id":"Ql3n1-T3rB-i"},"source":["def value_iteration(env, gamma=1.0, max_iterations = 100000):\n","    v = np.zeros(n_states)\n","    eps = 1e-20\n","    for i in range(max_iterations):\n","      prev_v = np.copy(v)\n","      \n","      for s in range(n_states):\n","        q_state_action = np.zeros(n_actions)\n","\n","        for a in range(n_actions):\n","          q_state_action_state = 0\n","\n","          for p, s_, r, _ in env.env.P[s][a]:\n","            q_state_action_state += p*(r + prev_v[s_])\n","          \n","          q_state_action[a] = q_state_action_state\n","\n","        v[s] = max(q_state_action)\n","      if (np.sum(np.fabs(prev_v - v)) <= eps):\n","          print ('Value-iteration converged at iteration# %d.' %(i+1))\n","          break\n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dfXMw3c-gks"},"source":["def value_iteration(env, gamma = 1.0):\n","    \"\"\" Value-iteration algorithm \"\"\"\n","    v = np.zeros(env.env.nS)  # initialize value-function\n","    max_iterations = 100000\n","    eps = 1e-20\n","    for i in range(max_iterations):\n","        prev_v = np.copy(v)\n","        for s in range(env.env.nS):\n","            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.env.nA)] \n","            v[s] = max(q_sa)\n","        if (np.sum(np.fabs(prev_v - v)) <= eps):\n","            print ('Value-iteration converged at iteration# %d.' %(i+1))\n","            break\n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XccGHZkErB-i"},"source":["Теперь мы можем найти оптимальную V-функцию, извлечь из нее оптимальную политику и оцениь ее."]},{"cell_type":"code","metadata":{"id":"tKkg6zzTrB-i"},"source":["optimal_v = value_iteration(env)\n","optimal_policy = extract_policy(optimal_v)\n","optimal_policy_score = evaluate_policy(env, optimal_policy, n=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nU7QpdbSrB-j"},"source":["print(optimal_v.reshape(4,4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBhHv_lXrB-j"},"source":["print(optimal_policy.reshape(4,4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYjOOHzlrB-j","executionInfo":{"status":"ok","timestamp":1649158059088,"user_tz":-180,"elapsed":244,"user":{"displayName":"Даниил Русланович Махоткин","userId":"18336248573599285046"}},"outputId":"557fca1c-5647-4d9e-a831-b8b6f803547a"},"source":["print(optimal_policy_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.75\n"]}]},{"cell_type":"markdown","metadata":{"id":"4JxyNSmhrB-j"},"source":["По сравнению со \"случайным\" агентом, который доходил до цели в 3 случаях из 100, наша новая политика позволяет добирться до цели в ~70% эпизодов."]},{"cell_type":"markdown","metadata":{"id":"LL0mKTRWrB-j"},"source":["## Policy Iteration"]},{"cell_type":"markdown","metadata":{"id":"BFpzQXlTrB-j"},"source":["Вспомним, что из себя представляет алгоритм Policy Iteration\n","![](https://drive.google.com/uc?export=view&id=1hphERFsRFKpNcBYTSVXgInMXO1WPXKT2)"]},{"cell_type":"markdown","metadata":{"id":"1Z0tz635rB-k"},"source":["Напишем необходимые вспомогательные функции."]},{"cell_type":"markdown","metadata":{"id":"IavoNuEGrB-k"},"source":["Начнем с основного цикла алгоритма, который вернет нам оптимальную политику."]},{"cell_type":"code","metadata":{"id":"vF24dYENrB-k"},"source":["def policy_iteration(env, gamma=1.0, max_iterations = 200000):\n","    policy = np.random.choice(n_actions, size=(n_states))  # initialize a random policy\n","    for i in range(max_iterations):\n","        old_policy_v = compute_policy_v(env, policy, gamma)\n","        new_policy = extract_policy(old_policy_v, gamma)\n","        if (np.all(policy == new_policy)):\n","            print ('Policy-Iteration converged at step %d.' %(i+1))\n","            break\n","        policy = new_policy\n","    return policy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMGkJ54brB-k"},"source":["Остается написать 2 функции, которые используются в основном цикле алгоритма Policy Iteration согласно псевдокоду."]},{"cell_type":"code","metadata":{"id":"GoxHkFhSrB-l"},"source":["def compute_policy_v(env, policy, gamma=1.0, eps=1e-10):\n","    v = np.zeros(n_states)\n","    # Your code goes here\n","    while True:\n","      prev_v = np.copy(v)\n","\n","      for s in range(n_states):\n","          policy_a = policy[s]\n","          v[s] = 0\n","          for p, s_, r, _ in env.env.P[s][policy_a]:\n","            v[s] += p * (r + gamma * prev_v[s_])\n","\n","      if (np.sum((np.fabs(prev_v - v))) <= eps):\n","          # value converged\n","          break\n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RnQ_YyXOrB-l"},"source":["def extract_policy(v, gamma=1.0):\n","    policy = np.zeros(n_states)\n","    # Your code goes here\n","    for s in range(n_states):\n","        q_sa = np.zeros(n_actions)\n","        for a in range(n_actions):\n","            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.env.P[s][a]])\n","        policy[s] = np.argmax(q_sa)\n","    return policy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLO7xE3urB-l"},"source":["Теперь мы также можем найти оптимальную V-функцию, извлечь из нее оптимальную политику и оцениь ее."]},{"cell_type":"code","metadata":{"id":"2TOzVMaMrB-l"},"source":["optimal_policy = policy_iteration(env)\n","optimal_policy_score = evaluate_policy(env, optimal_policy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqvQyBPnrB-l"},"source":["print(optimal_policy.reshape(4,4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkOoOmHqrB-l"},"source":["print(optimal_policy_score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?export=view&id=1OlzVdezDCeeNmcEl1VQg1DmlqkA6CCBZ)"],"metadata":{"id":"6FssGcsspd3Y"}}]}